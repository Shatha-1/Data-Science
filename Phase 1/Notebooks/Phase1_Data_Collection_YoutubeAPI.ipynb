{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip -q install requests tqdm\n",
        "\n",
        "import os, json, time, random\n",
        "import requests\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "k8LafWVy9UOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "YOUTUBE_API_KEY = \"\"\n",
        "\n",
        "RAW_DIR = \"raw_data\"\n",
        "os.makedirs(RAW_DIR, exist_ok=True)\n",
        "\n",
        "TARGET_ROWS = 5000"
      ],
      "metadata": {
        "id": "F4Esuigr-Cdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SESSION = requests.Session()\n",
        "\n",
        "def polite_sleep(min_s=0.2, max_s=0.8):\n",
        "    time.sleep(min_s + random.random() * (max_s - min_s))\n",
        "\n",
        "def yt_get(url, params, max_retries=5):\n",
        "    last = None\n",
        "    for attempt in range(max_retries):\n",
        "        polite_sleep()\n",
        "        r = SESSION.get(url, params=params, timeout=30)\n",
        "        last = r\n",
        "        if r.status_code == 200:\n",
        "            return r.json()\n",
        "        time.sleep(2 ** attempt)\n",
        "    raise RuntimeError(f\"Failed: {url} status={last.status_code} body={last.text[:200]}\")\n",
        "\n",
        "def append_jsonl(path, obj):\n",
        "    with open(path, \"a\", encoding=\"utf-8\") as f:\n",
        "        f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")"
      ],
      "metadata": {
        "id": "CJF1kIXn-YiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "QUERIES = [\n",
        "    \"flu vaccine\", \"flu shot\", \"influenza vaccine\", \"influenza shot\",\n",
        "    \"flu vaccine side effects\", \"flu shot side effects\",\n",
        "    \"annual flu shot\", \"flu jab\", \"flu vaccination\"\n",
        "]\n",
        "\n",
        "MAX_VIDEOS_PER_QUERY = 30\n",
        "COMMENTS_PER_VIDEO_CAP = 300"
      ],
      "metadata": {
        "id": "UTF-FMgZ_Qwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEARCH_URL = \"https://www.googleapis.com/youtube/v3/search\"\n",
        "\n",
        "def search_videos(query, max_videos=30):\n",
        "    video_ids = []\n",
        "    page_token = None\n",
        "\n",
        "    while len(video_ids) < max_videos:\n",
        "        params = {\n",
        "            \"key\": YOUTUBE_API_KEY,\n",
        "            \"part\": \"snippet\",\n",
        "            \"q\": query,\n",
        "            \"type\": \"video\",\n",
        "            \"maxResults\": 50\n",
        "        }\n",
        "        if page_token:\n",
        "            params[\"pageToken\"] = page_token\n",
        "\n",
        "        data = yt_get(SEARCH_URL, params)\n",
        "        items = data.get(\"items\", [])\n",
        "        for it in items:\n",
        "            vid = it.get(\"id\", {}).get(\"videoId\")\n",
        "            if vid:\n",
        "                video_ids.append(vid)\n",
        "            if len(video_ids) >= max_videos:\n",
        "                break\n",
        "\n",
        "        page_token = data.get(\"nextPageToken\")\n",
        "        if not page_token:\n",
        "            break\n",
        "\n",
        "    return list(dict.fromkeys(video_ids))\n",
        "\n",
        "# Collect video IDs across queries\n",
        "all_video_ids = []\n",
        "for q in QUERIES:\n",
        "    vids = search_videos(q, MAX_VIDEOS_PER_QUERY)\n",
        "    all_video_ids.extend(vids)\n",
        "\n",
        "\n",
        "all_video_ids = list(dict.fromkeys(all_video_ids))\n",
        "print(\"Total unique video IDs:\", len(all_video_ids))\n",
        "all_video_ids[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGjtf8dC_Uxq",
        "outputId": "4c792378-3bd6-44ac-a3ae-15c04ee34c09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total unique video IDs: 127\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Z2UqlSo3G-A',\n",
              " 'rnzuyLQkC6U',\n",
              " 'HfpGKLsFgjQ',\n",
              " 'DKByks4MbN4',\n",
              " 'KiEQTUFmSFQ',\n",
              " 'YHbvmOByiI0',\n",
              " '2h09oj26_H0',\n",
              " '4LNbKAatWvM',\n",
              " '1OutlE1Y0zg',\n",
              " 'YZCRkFL6qH0']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "COMMENT_THREADS_URL = \"https://www.googleapis.com/youtube/v3/commentThreads\"\n",
        "\n",
        "raw_comments_path = os.path.join(RAW_DIR, \"youtube_comments_raw.jsonl\")\n",
        "raw_videos_path = os.path.join(RAW_DIR, \"youtube_video_ids.jsonl\")\n",
        "\n",
        "# reset outputs\n",
        "open(raw_comments_path, \"w\").close()\n",
        "open(raw_videos_path, \"w\").close()\n",
        "\n",
        "# save video IDs as raw too\n",
        "for vid in all_video_ids:\n",
        "    append_jsonl(raw_videos_path, {\"videoId\": vid})\n",
        "\n",
        "count = 0\n",
        "seen_comment_ids = set()\n",
        "\n",
        "def fetch_comments_for_video(video_id, cap=300):\n",
        "    out = []\n",
        "    page_token = None\n",
        "\n",
        "    while len(out) < cap:\n",
        "        params = {\n",
        "            \"key\": YOUTUBE_API_KEY,\n",
        "            \"part\": \"snippet\",\n",
        "            \"videoId\": video_id,\n",
        "            \"maxResults\": 100,\n",
        "            \"textFormat\": \"plainText\"\n",
        "        }\n",
        "        if page_token:\n",
        "            params[\"pageToken\"] = page_token\n",
        "\n",
        "        data = yt_get(COMMENT_THREADS_URL, params)\n",
        "        items = data.get(\"items\", [])\n",
        "        if not items:\n",
        "            break\n",
        "\n",
        "        for it in items:\n",
        "            top = it.get(\"snippet\", {}).get(\"topLevelComment\", {})\n",
        "            cid = top.get(\"id\")\n",
        "            if not cid:\n",
        "                continue\n",
        "\n",
        "            # NO usernames saved\n",
        "            snippet = top.get(\"snippet\", {})\n",
        "            record = {\n",
        "                \"kind\": \"youtube#comment\",\n",
        "                \"commentId\": cid,\n",
        "                \"videoId\": video_id,\n",
        "                \"publishedAt\": snippet.get(\"publishedAt\"),\n",
        "                \"updatedAt\": snippet.get(\"updatedAt\"),\n",
        "                \"textDisplay\": snippet.get(\"textDisplay\"),\n",
        "                \"likeCount\": snippet.get(\"likeCount\"),\n",
        "            }\n",
        "            out.append(record)\n",
        "\n",
        "            if len(out) >= cap:\n",
        "                break\n",
        "\n",
        "        page_token = data.get(\"nextPageToken\")\n",
        "        if not page_token:\n",
        "            break\n",
        "\n",
        "    return out\n",
        "\n",
        "for vid in tqdm(all_video_ids):\n",
        "    if count >= TARGET_ROWS:\n",
        "        break\n",
        "\n",
        "    try:\n",
        "        rows = fetch_comments_for_video(vid, COMMENTS_PER_VIDEO_CAP)\n",
        "    except Exception as e:\n",
        "        continue\n",
        "\n",
        "    for r in rows:\n",
        "        if count >= TARGET_ROWS:\n",
        "            break\n",
        "        cid = r[\"commentId\"]\n",
        "        if cid in seen_comment_ids:\n",
        "            continue\n",
        "        seen_comment_ids.add(cid)\n",
        "\n",
        "        append_jsonl(raw_comments_path, r)\n",
        "        count += 1\n",
        "\n",
        "print(\"Total comment rows saved:\", count)\n",
        "print(\"Raw files:\", raw_comments_path, raw_videos_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXqxiwOb_i7d",
        "outputId": "503d17e7-e7f9-42d2-d59a-a571f672f01b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 127/127 [07:39<00:00,  3.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total comment rows saved: 3896\n",
            "Raw files: raw_data/youtube_comments_raw.jsonl raw_data/youtube_video_ids.jsonl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "print(\"Files in raw_data/:\")\n",
        "for fn in sorted(os.listdir(\"raw_data\")):\n",
        "    print(\"-\", fn, os.path.getsize(os.path.join(\"raw_data\", fn)), \"bytes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRCZJXw_VnwJ",
        "outputId": "bdc1bcdb-103f-40fb-c3d4-875090579eb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files in raw_data/:\n",
            "- youtube_comments_raw.jsonl 1391660 bytes\n",
            "- youtube_video_ids.jsonl 3429 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_lines(path):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return sum(1 for _ in f)\n",
        "\n",
        "print(\"YouTube comment rows:\", count_lines(\"raw_data/youtube_comments_raw.jsonl\"))\n",
        "print(\"Video IDs rows:\", count_lines(\"raw_data/youtube_video_ids.jsonl\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBm4CqSKV670",
        "outputId": "ee3ec49f-3075-44c0-9182-6b639a7a276d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YouTube comment rows: 3896\n",
            "Video IDs rows: 127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CSV export intentionally deffered to Phase 2 to keep Phase 1 focused on raw data collection."
      ],
      "metadata": {
        "id": "WvsZ8osmWtRf"
      }
    }
  ]
}